{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Data Analysis and Wrangling \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "# Visualization \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# Machine Learning  \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "# from sklearn.svm import SVC, LinearSVC\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.linear_model import Perceptron \n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics \n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path1 = '../Cleaned-Data/Training Data.xlsx'\n",
    "path2 = '../Cleaned-Data/Testing Data.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training = pd.read_excel(path1)\n",
    "df_testing = pd.read_excel(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average GPA</th>\n",
       "      <th>Coding</th>\n",
       "      <th>Logical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.5200</td>\n",
       "      <td>80.03</td>\n",
       "      <td>33.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.4900</td>\n",
       "      <td>64.47</td>\n",
       "      <td>49.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.4475</td>\n",
       "      <td>64.60</td>\n",
       "      <td>33.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.4100</td>\n",
       "      <td>50.76</td>\n",
       "      <td>67.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.0400</td>\n",
       "      <td>24.53</td>\n",
       "      <td>86.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Average GPA  Coding  Logical\n",
       "0       1.5200   80.03    33.53\n",
       "1       3.4900   64.47    49.67\n",
       "2       5.4475   64.60    33.40\n",
       "3       5.4100   50.76    67.41\n",
       "4       9.0400   24.53    86.73"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function for classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classification_model(model, data, predictors, outcome): \n",
    "    # Fit the model\n",
    "    model.fit(data[predictors], data[outcome])\n",
    "    \n",
    "    # Make predictions on training set: \n",
    "    predictions = model.predict(data[predictors])\n",
    "    \n",
    "    #print accuracy \n",
    "    accuracy = metrics.accuracy_score(predictions, data[outcome])\n",
    "    print \"Accuracy : %s\" % '{0: .3%}'.format(accuracy)\n",
    "    \n",
    "    # Perform k-fold validation with 5 folds\n",
    "    kf = KFold(data.shape[0], n_folds=5)\n",
    "    error = []\n",
    "    for train, test in kf:\n",
    "        # Filter training data\n",
    "        train_predictors = (data[predictors].iloc[train,:])\n",
    "        \n",
    "        # The target we're using to train the algorithm\n",
    "        train_target = data[outcome].iloc[train]\n",
    "        \n",
    "        # Training the algorithm using the predictors and target.\n",
    "        model.fit(train_predictors, train_target)\n",
    "        \n",
    "        # Record error from each cross-validation run\n",
    "        error.append(model.score(data[predictors].iloc[test,:], data[outcome].iloc[test]))\n",
    "        \n",
    "    print 'Cross-Validation Score: %s' % '{0: .3%}'.format(np.mean(error))\n",
    "        \n",
    "    # Fit the model again so that it can be refered outside the function:\n",
    "    model.fit(data[predictors], data[outcome])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  66.667%\n",
      "Cross-Validation Score:  65.608%\n"
     ]
    }
   ],
   "source": [
    "outcome_var = 'Placed 1/0'\n",
    "model = LogisticRegression()\n",
    "predictor_var = ['Average GPA','Coding','Logical']\n",
    "classification_model(model, df_training, predictor_var, outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------- Yet to work on above part --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_training.drop('Placed 1/0', axis=1)\n",
    "# Y_train = df_training['Placed 1/0']\n",
    "# X_test = df_testing.copy()\n",
    "# X_train.shape, Y_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifical Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((211, 3), (211,), (71, 3), (71,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, df_training['Placed 1/0'])\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()     # For standardization\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now apply the transformations to the data: \n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number shows the number of neurons in the nth layer of the MLP model\n",
    "# For simplicity keep this as the same number of neurons as there are features in the dataset\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(3,3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(3, 3, 3), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34  0]\n",
      " [37  0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      1.00      0.65        34\n",
      "          1       0.00      0.00      0.00        37\n",
      "\n",
      "avg / total       0.23      0.48      0.31        71\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN gives lower precision value compared to other algorithms because of size of dataset. This proves ML is useful for smaller datasets but for larger datasets ANN is useful as it improves over time with larger dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
